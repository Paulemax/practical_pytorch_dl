{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZDZAxV16L-d"
   },
   "source": [
    "# A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L4ocYX6Z6Qap"
   },
   "outputs": [],
   "source": [
    "# bring here the Value class code and update accordly, hint: follow other operations and loss function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F5-ffkTP6QXy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "  # POW SOLUTION:\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data ** other.data, (self, other), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other.data * self.data**(other-1).data) * out.grad\n",
    "            # update other gradient\n",
    "            other.grad += ((self.data ** other.data) * math.log(other.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "  # LOG SOLUTION:\n",
    "\n",
    "    def log(self):\n",
    "        # define out\n",
    "        # define backward function\n",
    "        # hint: see into another functions here...\n",
    "        out = Value(math.log(self.data), (self,), 'ln')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(2,_op=\"a\")\n",
    "b=Value(3,_op=\"b\")\n",
    "\n",
    "e=Value(math.e, _op=\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a**b\n",
    "d = c + e\n",
    "f = d.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2.3719508656009087, grad=0)\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a grad: 1.1195824286069578\n",
      "b grad: 0.8199913428296602\n",
      "c grad: 0.09329853571724647\n",
      "e grad: 0.09329853571724647\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"a grad: {a.grad}\")\n",
    "print(f\"b grad: {b.grad}\")\n",
    "print(f\"c grad: {c.grad}\")\n",
    "print(f\"e grad: {e.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS_M_TYr6SZR"
   },
   "source": [
    "# B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xgA2Vciq605Y"
   },
   "outputs": [],
   "source": [
    "from diabetes import prepare_data\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "64jsr6vP601I"
   },
   "outputs": [],
   "source": [
    "# uncomment to get data\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKVSFbF27N0V",
    "outputId": "66c30acc-847b-4d5a-81b1-b31221e26140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((331, 10), (111, 10), (331,), (111,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of the data to get a sense of what is in there\n",
    "# you should have around 300 samples for train, and 100 for test...\n",
    "# similarly, around 8 features, and 1 target.\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FzE7HF_6QPH",
    "outputId": "78b2849e-0ad7-4985-ae3b-e66c380ddc51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10), ReLUNeuron(10)], Layer of [LinearNeuron(20)]]\n",
      "number of parameters 241\n"
     ]
    }
   ],
   "source": [
    "# initialize a model (using components from nn.py)\n",
    "# insert correct arguments\n",
    "\n",
    "model = nn.MLP(10, [20, 1])\n",
    "\n",
    "print(model)                            \n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jgVAJIj46wlT"
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(x, y, batch_size=None, seed=0):\n",
    "\n",
    "    if batch_size is None:   # all data in a single batch\n",
    "      xb, yb = x, y \n",
    "    else:   # mini-batch\n",
    "      np.random.seed(seed)                 # randomly shuffle the data \n",
    "      ri = np.random.permutation(x.shape[0])\n",
    "      xb, yb = x[ri], y[ri]\n",
    "      \n",
    "      \n",
    "      total_batches = math.floor(xb.shape[0] / batch_size) # the total batches with given sample size\n",
    "      total_samples = total_batches * batch_size  # the total samples with sample size\n",
    "      xb = np.split(xb[:total_samples], total_batches, 0)  # we split by batches with the exact samples (indexing)\n",
    "      yb = np.split(yb[:total_samples], total_batches, 0)\n",
    "      xb, yb = np.array(xb), np.array(yb)  # we make back to an array\n",
    "\n",
    "      print(f'x with shape of {xb.shape}')\n",
    "      print(f'y with shape of {yb.shape}')\n",
    "\n",
    "      # we prepare the data in list of Values as required by micro-grad\n",
    "      xinputs = []\n",
    "      for x in xb:\n",
    "\n",
    "        xinput = [list(map(Value, xrow)) for xrow in x]\n",
    "        xinputs.append(xinput)\n",
    "\n",
    "      return xinputs, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Rw8NTZb08BBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x with shape of (20, 16, 10)\n",
      "y with shape of (20, 16)\n"
     ]
    }
   ],
   "source": [
    "x_train_loader, y_train_loader = get_dataloaders(X_train, y_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EEFHGwFNBEw6"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "print_loss_every_n_iteration = 1 \n",
    "total_iterations = epochs * len(x_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8cPl7D547VZM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 200 with a loss: 4294.530047312885\n",
      "iteration 2 of 200 with a loss: 5491.87322445898\n",
      "iteration 3 of 200 with a loss: 5103.273780020547\n",
      "iteration 4 of 200 with a loss: 6431.8943905617225\n",
      "iteration 5 of 200 with a loss: 3735.5899426852898\n",
      "iteration 6 of 200 with a loss: 5172.572700909284\n",
      "iteration 7 of 200 with a loss: 4465.56706356632\n",
      "iteration 8 of 200 with a loss: 5389.846677192501\n",
      "iteration 9 of 200 with a loss: 3519.030169301523\n",
      "iteration 10 of 200 with a loss: 4073.8577015071232\n",
      "iteration 11 of 200 with a loss: 4667.336794469692\n",
      "iteration 12 of 200 with a loss: 3447.8526290952973\n",
      "iteration 13 of 200 with a loss: 7353.914698414849\n",
      "iteration 14 of 200 with a loss: 6593.182679043102\n",
      "iteration 15 of 200 with a loss: 6740.87367111995\n",
      "iteration 16 of 200 with a loss: 4697.336830730052\n",
      "iteration 17 of 200 with a loss: 3122.65552722587\n",
      "iteration 18 of 200 with a loss: 3331.7234886880096\n",
      "iteration 19 of 200 with a loss: 8608.475943738005\n",
      "iteration 20 of 200 with a loss: 3682.026513860943\n",
      "iteration 21 of 200 with a loss: 5740.069412891666\n",
      "iteration 22 of 200 with a loss: 4957.797060525123\n",
      "iteration 23 of 200 with a loss: 4787.377479271463\n",
      "iteration 24 of 200 with a loss: 6100.720375234219\n",
      "iteration 25 of 200 with a loss: 3655.0056391129083\n",
      "iteration 26 of 200 with a loss: 4955.573333055676\n",
      "iteration 27 of 200 with a loss: 4264.713438285394\n",
      "iteration 28 of 200 with a loss: 5227.848380674263\n",
      "iteration 29 of 200 with a loss: 3318.399222635207\n",
      "iteration 30 of 200 with a loss: 3750.6489414810194\n",
      "iteration 31 of 200 with a loss: 4376.311984985557\n",
      "iteration 32 of 200 with a loss: 3336.9942770409307\n",
      "iteration 33 of 200 with a loss: 7155.771687873925\n",
      "iteration 34 of 200 with a loss: 6275.353185246268\n",
      "iteration 35 of 200 with a loss: 6437.999609202215\n",
      "iteration 36 of 200 with a loss: 4552.514704783226\n",
      "iteration 37 of 200 with a loss: 2917.0962578381145\n",
      "iteration 38 of 200 with a loss: 3179.7738029450834\n",
      "iteration 39 of 200 with a loss: 8296.518195916073\n",
      "iteration 40 of 200 with a loss: 3556.251739011405\n",
      "iteration 41 of 200 with a loss: 5507.85526882142\n",
      "iteration 42 of 200 with a loss: 4820.491021746356\n",
      "iteration 43 of 200 with a loss: 4598.342963565229\n",
      "iteration 44 of 200 with a loss: 5729.771572369528\n",
      "iteration 45 of 200 with a loss: 3594.274526641655\n",
      "iteration 46 of 200 with a loss: 4747.662863177316\n",
      "iteration 47 of 200 with a loss: 4090.344566243514\n",
      "iteration 48 of 200 with a loss: 5084.017435896726\n",
      "iteration 49 of 200 with a loss: 3131.9574044732353\n",
      "iteration 50 of 200 with a loss: 3434.3590571826917\n",
      "iteration 51 of 200 with a loss: 4090.4497221543547\n",
      "iteration 52 of 200 with a loss: 3232.7661587080215\n",
      "iteration 53 of 200 with a loss: 6967.968742263046\n",
      "iteration 54 of 200 with a loss: 5952.256822064913\n",
      "iteration 55 of 200 with a loss: 6134.351999605546\n",
      "iteration 56 of 200 with a loss: 4418.859742959297\n",
      "iteration 57 of 200 with a loss: 2714.287939495079\n",
      "iteration 58 of 200 with a loss: 3039.149899425789\n",
      "iteration 59 of 200 with a loss: 7992.403541828766\n",
      "iteration 60 of 200 with a loss: 3464.9756919400534\n",
      "iteration 61 of 200 with a loss: 5297.54844349619\n",
      "iteration 62 of 200 with a loss: 4691.436919110619\n",
      "iteration 63 of 200 with a loss: 4423.5482863826155\n",
      "iteration 64 of 200 with a loss: 5377.486083285743\n",
      "iteration 65 of 200 with a loss: 3559.3620408699426\n",
      "iteration 66 of 200 with a loss: 4546.446417130087\n",
      "iteration 67 of 200 with a loss: 3950.812125960723\n",
      "iteration 68 of 200 with a loss: 4965.639854356183\n",
      "iteration 69 of 200 with a loss: 2969.950798047329\n",
      "iteration 70 of 200 with a loss: 3134.6480257454355\n",
      "iteration 71 of 200 with a loss: 3820.1623032473763\n",
      "iteration 72 of 200 with a loss: 3137.8084513472327\n",
      "iteration 73 of 200 with a loss: 6796.564453250694\n",
      "iteration 74 of 200 with a loss: 5633.54633451803\n",
      "iteration 75 of 200 with a loss: 5839.8432429740305\n",
      "iteration 76 of 200 with a loss: 4299.193811457885\n",
      "iteration 77 of 200 with a loss: 2528.49926538469\n",
      "iteration 78 of 200 with a loss: 2916.082017864049\n",
      "iteration 79 of 200 with a loss: 7702.780609808537\n",
      "iteration 80 of 200 with a loss: 3417.338174897715\n",
      "iteration 81 of 200 with a loss: 5121.673938292264\n",
      "iteration 82 of 200 with a loss: 4574.081098658875\n",
      "iteration 83 of 200 with a loss: 4270.354927924636\n",
      "iteration 84 of 200 with a loss: 5055.254350280043\n",
      "iteration 85 of 200 with a loss: 3551.567650968729\n",
      "iteration 86 of 200 with a loss: 4357.828855785475\n",
      "iteration 87 of 200 with a loss: 3847.57083938676\n",
      "iteration 88 of 200 with a loss: 4877.495673734409\n",
      "iteration 89 of 200 with a loss: 2840.1404967054564\n",
      "iteration 90 of 200 with a loss: 2861.1216366537615\n",
      "iteration 91 of 200 with a loss: 3575.409686996884\n",
      "iteration 92 of 200 with a loss: 3053.5671090948513\n",
      "iteration 93 of 200 with a loss: 6645.432706300153\n",
      "iteration 94 of 200 with a loss: 5329.97938076462\n",
      "iteration 95 of 200 with a loss: 5564.302238730259\n",
      "iteration 96 of 200 with a loss: 4194.87171148491\n",
      "iteration 97 of 200 with a loss: 2372.483001229636\n",
      "iteration 98 of 200 with a loss: 2813.5810291782427\n",
      "iteration 99 of 200 with a loss: 7435.117191758405\n",
      "iteration 100 of 200 with a loss: 3415.670683795118\n",
      "iteration 101 of 200 with a loss: 4988.088293297574\n",
      "iteration 102 of 200 with a loss: 4470.428567580211\n",
      "iteration 103 of 200 with a loss: 4144.274703319185\n",
      "iteration 104 of 200 with a loss: 4772.145672786363\n",
      "iteration 105 of 200 with a loss: 3568.417963375283\n",
      "iteration 106 of 200 with a loss: 4185.591288225433\n",
      "iteration 107 of 200 with a loss: 3776.374648227462\n",
      "iteration 108 of 200 with a loss: 4818.695160052015\n",
      "iteration 109 of 200 with a loss: 2744.9903518184133\n",
      "iteration 110 of 200 with a loss: 2621.2204332591305\n",
      "iteration 111 of 200 with a loss: 3362.8459773596273\n",
      "iteration 112 of 200 with a loss: 2976.6687472829317\n",
      "iteration 113 of 200 with a loss: 6515.275061718108\n",
      "iteration 114 of 200 with a loss: 5050.779699546442\n",
      "iteration 115 of 200 with a loss: 5314.700226959161\n",
      "iteration 116 of 200 with a loss: 4104.50467131447\n",
      "iteration 117 of 200 with a loss: 2252.971996996711\n",
      "iteration 118 of 200 with a loss: 2731.01402888671\n",
      "iteration 119 of 200 with a loss: 7199.515617327159\n",
      "iteration 120 of 200 with a loss: 3452.8598707644232\n",
      "iteration 121 of 200 with a loss: 4897.792802233213\n",
      "iteration 122 of 200 with a loss: 4380.667759206318\n",
      "iteration 123 of 200 with a loss: 4047.32590666708\n",
      "iteration 124 of 200 with a loss: 4532.582704398375\n",
      "iteration 125 of 200 with a loss: 3603.670101364405\n",
      "iteration 126 of 200 with a loss: 4031.6239780286046\n",
      "iteration 127 of 200 with a loss: 3728.850399314432\n",
      "iteration 128 of 200 with a loss: 4783.987419190283\n",
      "iteration 129 of 200 with a loss: 2681.2590166838795\n",
      "iteration 130 of 200 with a loss: 2418.5550414021973\n",
      "iteration 131 of 200 with a loss: 3184.1402888607627\n",
      "iteration 132 of 200 with a loss: 2905.712920360918\n",
      "iteration 133 of 200 with a loss: 6403.1691582734675\n",
      "iteration 134 of 200 with a loss: 4802.019729960844\n",
      "iteration 135 of 200 with a loss: 5094.144301544917\n",
      "iteration 136 of 200 with a loss: 4030.4514043779172\n",
      "iteration 137 of 200 with a loss: 2169.861642908952\n",
      "iteration 138 of 200 with a loss: 2666.0650310937276\n",
      "iteration 139 of 200 with a loss: 7002.237374541648\n",
      "iteration 140 of 200 with a loss: 3516.3510716790306\n",
      "iteration 141 of 200 with a loss: 4843.885462203639\n",
      "iteration 142 of 200 with a loss: 4302.94984989099\n",
      "iteration 143 of 200 with a loss: 3978.2217092649817\n",
      "iteration 144 of 200 with a loss: 4336.483011453125\n",
      "iteration 145 of 200 with a loss: 3649.6870296771153\n",
      "iteration 146 of 200 with a loss: 3892.9644181971444\n",
      "iteration 147 of 200 with a loss: 3694.4746013749154\n",
      "iteration 148 of 200 with a loss: 4759.71571575061\n",
      "iteration 149 of 200 with a loss: 2641.5226660392645\n",
      "iteration 150 of 200 with a loss: 2252.709304460127\n",
      "iteration 151 of 200 with a loss: 3037.2046893739034\n",
      "iteration 152 of 200 with a loss: 2838.3388082669526\n",
      "iteration 153 of 200 with a loss: 6304.9755990828735\n",
      "iteration 154 of 200 with a loss: 4585.321880860184\n",
      "iteration 155 of 200 with a loss: 4902.4085694175155\n",
      "iteration 156 of 200 with a loss: 3964.9826457017784\n",
      "iteration 157 of 200 with a loss: 2115.8959877758575\n",
      "iteration 158 of 200 with a loss: 2615.162731753194\n",
      "iteration 159 of 200 with a loss: 6849.374461377781\n",
      "iteration 160 of 200 with a loss: 3593.3396058606318\n",
      "iteration 161 of 200 with a loss: 4815.808970420865\n",
      "iteration 162 of 200 with a loss: 4232.928192490115\n",
      "iteration 163 of 200 with a loss: 3932.944710470187\n",
      "iteration 164 of 200 with a loss: 4180.567007297213\n",
      "iteration 165 of 200 with a loss: 3697.963627124431\n",
      "iteration 166 of 200 with a loss: 3765.5256177382253\n",
      "iteration 167 of 200 with a loss: 3664.25206895907\n",
      "iteration 168 of 200 with a loss: 4745.2921394191335\n",
      "iteration 169 of 200 with a loss: 2616.781183061676\n",
      "iteration 170 of 200 with a loss: 2120.776214205564\n",
      "iteration 171 of 200 with a loss: 2918.0388269972805\n",
      "iteration 172 of 200 with a loss: 2774.9384741938984\n",
      "iteration 173 of 200 with a loss: 6214.52393975145\n",
      "iteration 174 of 200 with a loss: 4399.561208618079\n",
      "iteration 175 of 200 with a loss: 4736.425212605192\n",
      "iteration 176 of 200 with a loss: 3916.9032370933974\n",
      "iteration 177 of 200 with a loss: 2081.9746778528433\n",
      "iteration 178 of 200 with a loss: 2575.1741502169107\n",
      "iteration 179 of 200 with a loss: 6732.060326383409\n",
      "iteration 180 of 200 with a loss: 3673.2577165254143\n",
      "iteration 181 of 200 with a loss: 4800.0692019678845\n",
      "iteration 182 of 200 with a loss: 4170.868039735226\n",
      "iteration 183 of 200 with a loss: 3906.600450473376\n",
      "iteration 184 of 200 with a loss: 4057.9586117183794\n",
      "iteration 185 of 200 with a loss: 3742.1895909923605\n",
      "iteration 186 of 200 with a loss: 3648.898625480054\n",
      "iteration 187 of 200 with a loss: 3626.5244769664137\n",
      "iteration 188 of 200 with a loss: 4723.803476363654\n",
      "iteration 189 of 200 with a loss: 2593.765406863882\n",
      "iteration 190 of 200 with a loss: 2019.2217296764654\n",
      "iteration 191 of 200 with a loss: 2821.6300602492665\n",
      "iteration 192 of 200 with a loss: 2711.7357008506237\n",
      "iteration 193 of 200 with a loss: 6132.671236005992\n",
      "iteration 194 of 200 with a loss: 4244.604277285223\n",
      "iteration 195 of 200 with a loss: 4594.193130309528\n",
      "iteration 196 of 200 with a loss: 3873.8209205344174\n",
      "iteration 197 of 200 with a loss: 2055.9762601405782\n",
      "iteration 198 of 200 with a loss: 2543.0869471852634\n",
      "iteration 199 of 200 with a loss: 6653.987574959701\n",
      "iteration 200 of 200 with a loss: 3724.5131818887835\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "iteration_number = 0\n",
    "for epoch in range(epochs):      # for each epoch\n",
    "  for xtrain, ytrain in zip(x_train_loader, y_train_loader):  # for each iteration\n",
    "    scores = list(map(model, xtrain))   # we fit a batch into the model and get scores\n",
    "\n",
    "    # compute the MSE for all samples\n",
    "    losses = [(score - yt)**2 for score, yt in zip(scores, ytrain)]\n",
    "    \n",
    "    # get the mean of all the sample losses for the iteration here (using operations supported by the value class)\n",
    "    iteration_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    iteration_loss.backward()\n",
    "    for parameter in model.parameters():\n",
    "      parameter.data -= learning_rate * parameter.grad \n",
    "    iteration_number += 1 \n",
    "    if iteration_number % print_loss_every_n_iteration == 0:\n",
    "      print(f'iteration {iteration_number} of {total_iterations} with a loss: {iteration_loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wz8w269_Bi8p",
    "outputId": "e0b88298-08bc-42bd-9adf-d62b678d4249"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5216.971418367859"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test\n",
    "\n",
    "scores = list(map(model, X_test))\n",
    "\n",
    "# remember to update as per MSE \n",
    "results = [(score - yt)**2 for score, yt in zip(scores, ytrain)]\n",
    "results = [value.data for value in results]\n",
    "\n",
    "# look at your result and compare with the statistics below\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4eq7BJ4IpCh",
    "outputId": "75118821-77e8-4558-fa93-b3fe3f9ec64d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.0, 346.0, 154.09375, 77.63848891456801)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_loader.min(), y_train_loader.max(), y_train_loader.mean(), y_train_loader.std()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cd52e3fcb9baaa40b3d605a6296e70b2d59f0509251b8daccec7a45dc12b452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
