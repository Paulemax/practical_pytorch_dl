{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_norm_2d(X, gamma, beta, moving_mean, moving_var, train_mode, eps=1e-5, momentum=0.1):\n",
    "    \"\"\"eps and momentum values are the same as pytorch defaults\"\"\"\n",
    "\n",
    "    if not train_mode:\n",
    "        X_hat = torch.div(X - moving_mean[None, ::, None, None], torch.sqrt(moving_var[None, ::, None, None] + eps))\n",
    "    else:\n",
    "        # we need to set unbiased=False to remove the bezel correction \n",
    "        mean = X.mean(dim=(0, 2, 3)) \n",
    "        var = X.var(dim=(0, 2, 3), unbiased=False) \n",
    "        X_hat = torch.div(X - mean[None, ::, None, None], torch.sqrt(var[None, ::, None, None] + eps))\n",
    "\n",
    "        # update moving_mean and var\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    \n",
    "    norm_batch = X_hat * gamma + beta\n",
    "    return norm_batch, moving_mean, moving_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNorm2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super().__init__()\n",
    "        shape = (1, in_channels, 1, 1)\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(shape))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(in_channels)\n",
    "        self.moving_var = torch.zeros(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.training is set by self.eval() and self.train()\n",
    "        norm, mm, mv = batch_norm_2d(x, self.gamma, self.beta, self.moving_mean, self.moving_var, self.training)\n",
    "        self.moving_mean = mm\n",
    "        self.mv = mv\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mb_size = 16, 3 channels and 12 x 12 image size\n",
    "to_norm = torch.randn((2, 3, 4, 4))\n",
    "shape = (1, 3, 1, 1)\n",
    "gamma  = torch.ones(shape)\n",
    "beta  = torch.zeros(shape)\n",
    "moving_mean = torch.zeros(3)\n",
    "moving_var = torch.zeros(3)\n",
    "eps = 1e-5\n",
    "\n",
    "bn2d = MyBatchNorm2d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6874, -0.2554, -0.2423,  0.1149],\n",
      "         [ 1.0755, -0.1256,  1.4470, -0.3999],\n",
      "         [ 0.3599, -0.0605, -1.5511, -0.2335],\n",
      "         [ 1.3836, -0.4995, -0.9170,  0.5237]],\n",
      "\n",
      "        [[-1.6499, -2.2293, -0.1241,  0.1928],\n",
      "         [-0.4826, -1.3876, -0.8286,  1.0646],\n",
      "         [ 0.6182,  2.1962, -0.3039, -0.7923],\n",
      "         [ 1.8773, -0.9112,  0.8893,  0.2331]],\n",
      "\n",
      "        [[-1.7067, -1.9626, -1.0487,  0.8881],\n",
      "         [ 0.0134,  0.5152,  0.1008,  0.3373],\n",
      "         [ 0.5981,  0.6143, -0.9635, -1.1860],\n",
      "         [-0.4608,  2.1036, -0.3587,  0.9629]]]) tensor([[[ 0.6874, -0.2554, -0.2423,  0.1149],\n",
      "         [ 1.0755, -0.1256,  1.4470, -0.3999],\n",
      "         [ 0.3599, -0.0605, -1.5511, -0.2335],\n",
      "         [ 1.3836, -0.4995, -0.9170,  0.5237]],\n",
      "\n",
      "        [[-1.6499, -2.2293, -0.1241,  0.1928],\n",
      "         [-0.4826, -1.3876, -0.8286,  1.0646],\n",
      "         [ 0.6182,  2.1962, -0.3039, -0.7923],\n",
      "         [ 1.8773, -0.9112,  0.8893,  0.2331]],\n",
      "\n",
      "        [[-1.7067, -1.9626, -1.0487,  0.8881],\n",
      "         [ 0.0134,  0.5152,  0.1008,  0.3373],\n",
      "         [ 0.5981,  0.6143, -0.9635, -1.1860],\n",
      "         [-0.4608,  2.1036, -0.3587,  0.9629]]], grad_fn=<SelectBackward0>) tensor([[[ 0.6874, -0.2554, -0.2423,  0.1149],\n",
      "         [ 1.0755, -0.1256,  1.4470, -0.3999],\n",
      "         [ 0.3599, -0.0605, -1.5511, -0.2335],\n",
      "         [ 1.3836, -0.4995, -0.9170,  0.5237]],\n",
      "\n",
      "        [[-1.6499, -2.2293, -0.1241,  0.1928],\n",
      "         [-0.4826, -1.3876, -0.8286,  1.0646],\n",
      "         [ 0.6182,  2.1962, -0.3039, -0.7923],\n",
      "         [ 1.8773, -0.9112,  0.8893,  0.2331]],\n",
      "\n",
      "        [[-1.7067, -1.9626, -1.0487,  0.8881],\n",
      "         [ 0.0134,  0.5152,  0.1008,  0.3373],\n",
      "         [ 0.5981,  0.6143, -0.9635, -1.1860],\n",
      "         [-0.4608,  2.1036, -0.3587,  0.9629]]])\n"
     ]
    }
   ],
   "source": [
    "# Check if batch norms to the same by hand\n",
    "\n",
    "import torch.nn.functional as F\n",
    "a, b, c = batch_norm_2d(X=to_norm, gamma=gamma, beta=beta, moving_mean=moving_mean, moving_var=moving_var, train_mode=True)\n",
    "a_ = bn2d(to_norm)\n",
    "bns = F.batch_norm(to_norm, weight=gamma, bias=beta, running_mean=moving_mean, running_var=moving_var, training=True)\n",
    "\n",
    "print(a[0], a_[0], bns[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and Paste from last exercise \n",
    "import pytorch_lightning as pl\n",
    "from typing import Any\n",
    "import torchmetrics\n",
    "import torch\n",
    "\n",
    "class CNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, loss: callable, lr: float, conv_layers, classification_head:torch.nn.Module ,num_classes:int =10) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.classification_head = classification_head \n",
    "        self.num_classes = num_classes\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.test_auroc = torchmetrics.AUROC(num_classes=self.num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classification_head(x)\n",
    "        return x\n",
    "\n",
    "    def _step(self, batch) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.loss(pred, y)\n",
    "        return pred, loss\n",
    "\n",
    "    def training_step(self, batch) -> torch.Tensor:\n",
    "        pred, loss = self._step(batch)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        acc = torchmetrics.functional.accuracy(pred, batch[-1], num_classes=self.num_classes)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def _eval_step(self, batch, auroc, acc):\n",
    "        pred, loss = self._step(batch)\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        auroc.update(pred, batch[-1])\n",
    "        acc.update(pred, batch[-1])\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        loss = self._eval_step(batch, self.test_auroc, self.test_acc)\n",
    "        self.log(\"test\", loss)\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> None:\n",
    "        print(f\"Test AUROC: {self.test_auroc.compute().data}\")\n",
    "        print(f\"Test Accuracy: {self.test_acc.compute().data}\")\n",
    "\n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        loss = self._eval_step(batch, self.test_auroc, self.test_acc)\n",
    "        self.log(\"test\", loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> None:\n",
    "        print(f\"Test AUROC: {self.test_auroc.compute().data}\")\n",
    "        print(f\"Test Accuracy: {self.test_acc.compute().data}\")\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),]\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root=\"data\", \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform)\n",
    "\n",
    "dl_test = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "num_classes = 10\n",
    "\n",
    "conv_layers_my_bn = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, (3, 3)),\n",
    "    MyBatchNorm2d(32)\n",
    ")\n",
    "\n",
    "conv_layers_torch_bn = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, (3, 3)),\n",
    "    torch.nn.BatchNorm2d(32)\n",
    ")\n",
    "\n",
    "pool_head = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32, num_classes),\n",
    ")\n",
    "\n",
    "pool_head_ = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32, num_classes),\n",
    ")\n",
    "\n",
    "custom_bn_model = CNN(loss, lr, conv_layers_my_bn, pool_head, num_classes)\n",
    "\n",
    "torch_bn_model = CNN(loss, lr, conv_layers_torch_bn, pool_head_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | conv_layers         | Sequential       | 384   \n",
      "1 | classification_head | Sequential       | 330   \n",
      "2 | loss                | CrossEntropyLoss | 0     \n",
      "3 | test_auroc          | AUROC            | 0     \n",
      "4 | test_acc            | Accuracy         | 0     \n",
      "---------------------------------------------------------\n",
      "714       Trainable params\n",
      "0         Non-trainable params\n",
      "714       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddd1123a092469aabb872d483bbf201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | conv_layers         | Sequential       | 384   \n",
      "1 | classification_head | Sequential       | 330   \n",
      "2 | loss                | CrossEntropyLoss | 0     \n",
      "3 | test_auroc          | AUROC            | 0     \n",
      "4 | test_acc            | Accuracy         | 0     \n",
      "---------------------------------------------------------\n",
      "714       Trainable params\n",
      "0         Non-trainable params\n",
      "714       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcab42731f3043c59fa2bcdc5468f600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "# I train and test on the test set, because the goal is to show that the custom batch_norm layer behaves the same \n",
    "# Numbers can vary, because initialization of weights and biases can vary\n",
    "\n",
    "trainer_custom_bn = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=10)\n",
    "trainer_custom_bn.fit(custom_bn_model, dl_test)\n",
    "\n",
    "trainer_torch_bn = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=10)\n",
    "trainer_torch_bn.fit(torch_bn_model, dl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2088689ad81487b8327ef0f4ee26196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.622795045375824\n",
      "Test Accuracy: 0.19509999454021454\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          test               50.35130310058594\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927a6cd188d74945a0feee4777c7c8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.666571855545044\n",
      "Test Accuracy: 0.2069000005722046\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          test               2.100219488143921\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test': 2.100219488143921}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_custom_bn.test(custom_bn_model, dl_test)\n",
    "trainer_torch_bn.test(torch_bn_model, dl_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cd52e3fcb9baaa40b3d605a6296e70b2d59f0509251b8daccec7a45dc12b452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
