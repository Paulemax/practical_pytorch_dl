{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_norm_2d(X, gamma, beta, moving_mean, moving_var, train_mode, eps=1e-5, momentum=0.1):\n",
    "    \"\"\"eps and momentum values are the same as pytorch defaults\"\"\"\n",
    "    def _bn(x, mean, var):\n",
    "        # expansion for our 1d mean and var variables\n",
    "        _expansion = (1, mean.shape[0], 1, 1)\n",
    "        # according to the batchnorm paper eps is added inside the sqrt instead of after as shown in the lecture\n",
    "        # There should not be any significant error from that but just to document it\n",
    "        return torch.div(x - mean.view(_expansion), torch.sqrt(var.view(_expansion) + eps)) * gamma + beta\n",
    "\n",
    "    if not train_mode:\n",
    "        return _bn(X, moving_mean, moving_var), moving_mean, moving_var\n",
    "\n",
    "    # we need to set unbiased=False to remove the bezel correction \n",
    "    mean = X.mean(dim=(0, 2, 3)) \n",
    "    var = X.var(dim=(0, 2, 3), unbiased=False) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        moving_mean.mul_(1.0 - momentum).add_(momentum * mean)\n",
    "        moving_var.mul_(1.0 - momentum).add_(momentum * var)\n",
    "    \n",
    "    return _bn(X, mean, var), moving_mean, moving_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNorm2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super().__init__()\n",
    "        shape = (1, in_channels, 1, 1)\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(shape))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.nn.Parameter(torch.zeros(in_channels))\n",
    "        self.moving_var = torch.nn.Parameter(torch.zeros(in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.training is set by self.eval() and self.train()\n",
    "        # we update moving mean and var inplace, since it has to be a parameter \n",
    "        # this destroys batch_norm_2d purity ;(\n",
    "        norm, _, _ = batch_norm_2d(\n",
    "            X=x, \n",
    "            gamma=self.gamma, \n",
    "            beta=self.beta, \n",
    "            moving_mean=self.moving_mean, \n",
    "            moving_var=self.moving_var, \n",
    "            train_mode=self.training\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.6069,  1.4866,  0.2035,  0.1400],\n",
      "         [ 0.8278, -0.6942, -0.4111, -1.8127],\n",
      "         [ 0.0092,  0.6123, -1.1293, -0.4382],\n",
      "         [ 0.2795,  0.1995,  1.6047, -0.8003]],\n",
      "\n",
      "        [[-0.3410, -0.9540,  0.3204,  0.3103],\n",
      "         [-0.8923, -1.6469, -0.5020,  1.1118],\n",
      "         [-0.4890,  2.8161,  0.1520,  0.3219],\n",
      "         [-0.2665,  1.3162,  0.4014,  1.6630]],\n",
      "\n",
      "        [[ 0.2422, -0.7985, -0.2573,  0.0778],\n",
      "         [-0.0048,  1.1459, -0.2289,  1.3111],\n",
      "         [-0.5052, -1.4229,  0.9413,  0.0635],\n",
      "         [-0.5695,  1.6662,  1.3684, -0.9769]]]) tensor([[[-1.6069,  1.4866,  0.2035,  0.1400],\n",
      "         [ 0.8278, -0.6942, -0.4111, -1.8127],\n",
      "         [ 0.0092,  0.6123, -1.1293, -0.4382],\n",
      "         [ 0.2795,  0.1995,  1.6047, -0.8003]],\n",
      "\n",
      "        [[-0.3410, -0.9540,  0.3204,  0.3103],\n",
      "         [-0.8923, -1.6469, -0.5020,  1.1118],\n",
      "         [-0.4890,  2.8161,  0.1520,  0.3219],\n",
      "         [-0.2665,  1.3162,  0.4014,  1.6630]],\n",
      "\n",
      "        [[ 0.2422, -0.7985, -0.2573,  0.0778],\n",
      "         [-0.0048,  1.1459, -0.2289,  1.3111],\n",
      "         [-0.5052, -1.4229,  0.9413,  0.0635],\n",
      "         [-0.5695,  1.6662,  1.3684, -0.9769]]], grad_fn=<SelectBackward0>) tensor([[[-1.6069,  1.4866,  0.2035,  0.1400],\n",
      "         [ 0.8278, -0.6942, -0.4111, -1.8127],\n",
      "         [ 0.0092,  0.6123, -1.1293, -0.4382],\n",
      "         [ 0.2795,  0.1995,  1.6047, -0.8003]],\n",
      "\n",
      "        [[-0.3410, -0.9540,  0.3204,  0.3103],\n",
      "         [-0.8923, -1.6469, -0.5020,  1.1118],\n",
      "         [-0.4890,  2.8161,  0.1520,  0.3219],\n",
      "         [-0.2665,  1.3162,  0.4014,  1.6630]],\n",
      "\n",
      "        [[ 0.2422, -0.7985, -0.2573,  0.0778],\n",
      "         [-0.0048,  1.1459, -0.2289,  1.3111],\n",
      "         [-0.5052, -1.4229,  0.9413,  0.0635],\n",
      "         [-0.5695,  1.6662,  1.3684, -0.9769]]])\n",
      "tensor([ 0.0062, -0.0130,  0.0182]) Parameter containing:\n",
      "tensor([ 0.0062, -0.0130,  0.0182], requires_grad=True) tensor([ 0.0062, -0.0130,  0.0182])\n"
     ]
    }
   ],
   "source": [
    "# Check if batch norms do the same thing \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# mb_size = 2, 3 channels and 4 x 4 image size\n",
    "to_norm = torch.randn((2, 3, 4, 4))\n",
    "shape = (1, 3, 1, 1)\n",
    "gamma  = torch.ones(shape)\n",
    "beta  = torch.zeros(shape)\n",
    "moving_mean = torch.zeros(3)\n",
    "moving_var = torch.zeros(3)\n",
    "eps = 1e-5\n",
    "\n",
    "bn2d = MyBatchNorm2d(3)\n",
    "\n",
    "a, b, c = batch_norm_2d(X=to_norm, gamma=gamma, beta=beta, moving_mean=moving_mean, moving_var=moving_var, train_mode=True)\n",
    "a_ = bn2d(to_norm)\n",
    "# we need to reset moving_mean and var because we took away batch_norm_2d's purity \n",
    "moving_mean = torch.zeros(3)\n",
    "moving_var = torch.zeros(3)\n",
    "bns = F.batch_norm(to_norm, weight=gamma, bias=beta, running_mean=moving_mean, running_var=moving_var, training=True)\n",
    "\n",
    "# check if the results are the same\n",
    "print(a[0], a_[0], bns[0]) \n",
    "\n",
    "# check if moving_mean is updated the same way\n",
    "print(b, bn2d.moving_mean, moving_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and Paste from last exercise \n",
    "import pytorch_lightning as pl\n",
    "from typing import Any\n",
    "import torchmetrics\n",
    "import torch\n",
    "\n",
    "class CNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, loss: callable, lr: float, conv_layers, classification_head:torch.nn.Module ,num_classes:int =10) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.classification_head = classification_head \n",
    "        self.num_classes = num_classes\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.test_auroc = torchmetrics.AUROC(num_classes=self.num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classification_head(x)\n",
    "        return x\n",
    "\n",
    "    def _step(self, batch) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.loss(pred, y)\n",
    "        return pred, loss\n",
    "\n",
    "    def training_step(self, batch) -> torch.Tensor:\n",
    "        pred, loss = self._step(batch)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        acc = torchmetrics.functional.accuracy(pred, batch[-1], num_classes=self.num_classes)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def _eval_step(self, batch, auroc, acc):\n",
    "        pred, loss = self._step(batch)\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        auroc.update(pred, batch[-1])\n",
    "        acc.update(pred, batch[-1])\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        loss = self._eval_step(batch, self.test_auroc, self.test_acc)\n",
    "        self.log(\"test\", loss)\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> None:\n",
    "        print(f\"Test AUROC: {self.test_auroc.compute().data}\")\n",
    "        print(f\"Test Accuracy: {self.test_acc.compute().data}\")\n",
    "\n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        loss = self._eval_step(batch, self.test_auroc, self.test_acc)\n",
    "        self.log(\"test\", loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> None:\n",
    "        print(f\"Test AUROC: {self.test_auroc.compute().data}\")\n",
    "        print(f\"Test Accuracy: {self.test_acc.compute().data}\")\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),]\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root=\"data\", \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform)\n",
    "\n",
    "dl_test = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "num_classes = 10\n",
    "\n",
    "conv_layers_my_bn = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, (3, 3)),\n",
    "    MyBatchNorm2d(32)\n",
    ")\n",
    "\n",
    "conv_layers_torch_bn = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, (3, 3)),\n",
    "    torch.nn.BatchNorm2d(32)\n",
    ")\n",
    "\n",
    "pool_head = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32, num_classes),\n",
    ")\n",
    "\n",
    "pool_head_ = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32, num_classes),\n",
    ")\n",
    "\n",
    "custom_bn_model = CNN(loss, lr, conv_layers_my_bn, pool_head, num_classes)\n",
    "\n",
    "torch_bn_model = CNN(loss, lr, conv_layers_torch_bn, pool_head_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | conv_layers         | Sequential       | 448   \n",
      "1 | classification_head | Sequential       | 330   \n",
      "2 | loss                | CrossEntropyLoss | 0     \n",
      "3 | test_auroc          | AUROC            | 0     \n",
      "4 | test_acc            | Accuracy         | 0     \n",
      "---------------------------------------------------------\n",
      "778       Trainable params\n",
      "0         Non-trainable params\n",
      "778       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66dc78721fb40089675d78f2de1b6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | conv_layers         | Sequential       | 384   \n",
      "1 | classification_head | Sequential       | 330   \n",
      "2 | loss                | CrossEntropyLoss | 0     \n",
      "3 | test_auroc          | AUROC            | 0     \n",
      "4 | test_acc            | Accuracy         | 0     \n",
      "---------------------------------------------------------\n",
      "714       Trainable params\n",
      "0         Non-trainable params\n",
      "714       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca40e393cc64074935c15a239efc37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "# I train and test on the test set, because the goal is to show that the custom batch_norm layer behaves the same \n",
    "# Numbers can vary, because initialization of weights and biases can vary\n",
    "\n",
    "trainer_custom_bn = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=10)\n",
    "trainer_custom_bn.fit(custom_bn_model, dl_test)\n",
    "\n",
    "trainer_torch_bn = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=10)\n",
    "trainer_torch_bn.fit(torch_bn_model, dl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2088689ad81487b8327ef0f4ee26196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.622795045375824\n",
      "Test Accuracy: 0.19509999454021454\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          test               50.35130310058594\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/workspaces/PycharmProjects/practical_pytorch_dl/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927a6cd188d74945a0feee4777c7c8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.666571855545044\n",
      "Test Accuracy: 0.2069000005722046\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          test               2.100219488143921\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test': 2.100219488143921}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_custom_bn.test(custom_bn_model, dl_test)\n",
    "trainer_torch_bn.test(torch_bn_model, dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part c observations:\n",
    "\n",
    "- there is no significant difference between the torch implementation and my implementation.\n",
    "- the difference is probably because of different initialization of the Parameters and other random factors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cd52e3fcb9baaa40b3d605a6296e70b2d59f0509251b8daccec7a45dc12b452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
